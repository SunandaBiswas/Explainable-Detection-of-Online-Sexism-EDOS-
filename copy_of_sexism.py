# -*- coding: utf-8 -*-
"""Copy of Sexism.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16bcMPVk922sFUrnWenZCDmDfVjT5HNHD

# Load the Data
"""

from google.colab import files
import pandas as pd

uploaded = files.upload()
file_name = list(uploaded.keys())[0]  # change the file name if necessary

# Load the data into a DataFrame
df = pd.read_csv(file_name, delimiter=',')  # Ensure the delimiter matches your file's format

"""#Data Cleaning"""

# Display basic info about the dataframe to understand any missing values or incorrect datatypes
print(df.info())

# Display the first few rows of the dataframe
print(df.head())

# Check for and sum up any missing values by column
print(df.isnull().sum())

# Drop rows with any missing 'text' or 'label_sexist' data
if df.isnull().sum().any():
    df.dropna(subset=['text', 'label_sexist'], inplace=True)


# # Optional: Reset index after dropping rows
# df.reset_index(drop=True, inplace=True)

"""#Step 3: Text Preprocessing"""

df.columns

import re
from nltk.stem import PorterStemmer
import string
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

# Function to preprocess text
def clean_text(text):

    # convert to lowercase
    text = str(text).lower()

    # remove single characters
    text = re.sub(pattern=r'\s+[a-zA-Z]\s+',repl='',string = text)

    # Remove URls, whitespace characters
    text = re.sub(r'https?://\S+|www\.\S+',repl='',string = text)

    # Removes all the special characters, digits from 0-9 and Capital Letters
    text = re.sub(r'[^a-z\s]',' ',string = text)

    # Substituting multiple spaces with single space
    text = re.sub(r'\s+', ' ', string = text)

    # create stemming object
    ps = PorterStemmer()
    text = [ps.stem(word) for word in text.split() if not word in set(stopwords.words('english'))]
    text = ' '.join(text)
    return text

# Check if 'text' column exists in DataFrame
if 'text' in df.columns:
    # Apply preprocessing to each text entry
    df['text'] = df['text'].apply(clean_text)
    # Check the processed text
    print(df.head())
else:
    print("'text' column does not exist in the DataFrame.")

"""##Exploratory Data Analysis (EDA)
#Class Distribution
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Class distribution
class_distribution = df['label_sexist'].value_counts()
print(class_distribution)

# Plot the distribution
sns.countplot(x=df['label_sexist'])
plt.title('Class Distribution of Sexist and Not Sexist Labels')
plt.xlabel('Sexist Label')
plt.ylabel('Frequency')
plt.show()

"""# Text Analysis
* Common Words by Category
"""

from wordcloud import WordCloud

# Function to generate a word cloud
def generate_word_cloud(text, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(str(text))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud)
    plt.axis('off')
    plt.title(title)
    plt.show()

# Generate word clouds for each category
sexist_text = df[df['label_sexist'] == 'sexist']['text']
not_sexist_text = df[df['label_sexist'] == 'not sexist']['text']

generate_word_cloud(sexist_text, 'Common Words in Sexist Texts')
generate_word_cloud(not_sexist_text, 'Common Words in Not Sexist Texts')

"""#Average Length of Texts


"""

# Calculate average length of the texts
df['text_length'] = df['text'].apply(len)
average_length_sexist = df[df['label_sexist'] == 'sexist']['text_length'].mean()
average_length_not_sexist = df[df['label_sexist'] == 'not sexist']['text_length'].mean()

print(f"Average length of sexist texts: {average_length_sexist:.2f}")
print(f"Average length of not sexist texts: {average_length_not_sexist:.2f}")

# Boxplot for text length distribution by category
sns.boxplot(x='label_sexist', y='text_length', data=df)
plt.title('Text Length Distribution by Category')
plt.xlabel('Sexist Label')
plt.ylabel('Text Length')
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer

# TFidf Vectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf=TfidfVectorizer(max_features=5000,ngram_range=(1,2))
X=tfidf.fit_transform(df['text']).toarray()

#tfidf.get_feature_names()[:50]

tfidf.get_params()
tfidf.vocabulary_

"""##Model Selection - LSTM
Text Vectorization,
Split the Data,
Model Building
"""

import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D
from tensorflow.keras.optimizers import Adam

# Parameters
vocab_size = 10000  # Number of words to consider as features
max_len = 100       # Cut texts after this number of words
embedding_dim = 50  # Dimensionality of the embedding space
oov_tok = "<OOV>"   # Out of Vocabulary token

# Step 1: Tokenize text
tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(df['text'])
word_index = tokenizer.word_index

# Convert text to sequence of integers
sequences = tokenizer.texts_to_sequences(df['text'])
padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')

# Prepare target variable
labels = np.where(df['label_sexist'] == 'sexist', 1, 0)

# Step 2: Split the data
X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2

# Step 3: Build the LSTM model
model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_len),
    SpatialDropout1D(0.2),
    LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
model.summary()

# Model Training
history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val), verbose=2)

"""#Model Evaluation"""

# Evaluate the model on the test set
test_performance = model.evaluate(X_test, y_test, verbose=2)
print(f'Test Loss: {test_performance[0]}, Test Accuracy: {test_performance[1]}')

"""#Model Performance Evaluation
Performance Metrics,
Confusion Matrix
"""

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# Predictions
y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5).astype(int)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
print(classification_report(y_test, y_pred))

"""#Visualizations
Training History
"""

import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()